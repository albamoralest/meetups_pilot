{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4921149-8064-4594-8b85-babce5c8d76a",
   "metadata": {},
   "source": [
    "### identify PPE: Places, Persons, Events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76383e8-c8d5-4111-95a9-9c01e9a40e5a",
   "metadata": {},
   "source": [
    "### 1. Identify Places, Persons, Events using DBpedia Spotlight\n",
    "### 2. Identify time entities using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a0900ea-2fc1-4e27-8259-cc2b432db572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For DBpedia spotlight, PPE entities\n",
    "import requests\n",
    "import pycurl\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from _datetime import date\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7b04f3c-1504-4d1f-8069-7e7d31a4aec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For nltk time entities\n",
    "# time entity\n",
    "import nltk.tokenize as nt\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import Tree\n",
    "\n",
    "# if not installed\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de1ca3a4-8c9f-459f-839d-b696f941c7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1 entries, 3 to 3\n",
      "Data columns (total 1 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   file_name  1 non-null      object\n",
      "dtypes: object(1)\n",
      "memory usage: 16.0+ bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10085.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   file_name\n",
       "3  10085.csv"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading every CSV with indexed sentences\n",
    "# return a list object of files in the given folder\n",
    "files_list = [f for f in os.listdir('indexedSentences') if not f.startswith('.')]\n",
    "# parse to dataframe\n",
    "df_files = pd.DataFrame(files_list, columns=['file_name'])\n",
    "df_files = df_files.query(\"file_name=='10085.csv'\")\n",
    "\n",
    "df_files.info()\n",
    "df_files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bfb2ab8-9aeb-434b-a401-04a73eff7b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 0 entries\n",
      "Data columns (total 1 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   file_name  0 non-null      object\n",
      "dtypes: object(1)\n",
      "memory usage: 0.0+ bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [file_name]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract only the ones that do not exist in folder\n",
    "files_list = [f for f in os.listdir('extractedEntities') if not f.startswith('.')]\n",
    "# parse to dataframe\n",
    "df_query = pd.DataFrame(files_list, columns=['file_name'])\n",
    "df_result = df_files[~df_files['file_name'].isin(df_query['file_name'])]\n",
    "df_files = df_result\n",
    "df_files.info()\n",
    "df_files.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfaca6f-7b2c-4078-a7b6-615e2a551cda",
   "metadata": {},
   "source": [
    "## 1. Identify PPE using DBpedia Spotlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33917612-c4ef-4232-b4c7-74aef72d5b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using DBPedia spotlight to search for entities\n",
    "# URL for local installation of DBpedia spotlight\n",
    "# urlAnnotation = 'http://dbpedia-spotlight.en:80/rest/annotate/'\n",
    "urlAnnotation = 'https://whise.kmi.open.ac.uk/rest/annotate'\n",
    "\n",
    "# setting headers and parameters not using DBpedia categories\n",
    "def setDbPediaAnnotationServiceParameters(text):\n",
    "    \"\"\" Se parameters for querying Dbpedia\n",
    "    args: text - text to be analysed\n",
    "    return: headers\n",
    "            params\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'Accept':'application/json',\n",
    "        \"content-type\":\"application/x-www-form-urlencoded\"\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        'confidence': '0.4',\n",
    "        \"text\":text,\n",
    "    }\n",
    "    return headers, params\n",
    "\n",
    "# setting headers and parameters, filtering by categories\n",
    "def setDbPediaAnnotationServiceParametersTypes(text,types):\n",
    "    \"\"\" Se parameters for querying Dbpedia\n",
    "    args: text - text to be analysed\n",
    "        types: different categories of entities\n",
    "    return: headers\n",
    "            params\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'Accept':'application/json',\n",
    "        \"content-type\":\"application/x-www-form-urlencoded\"\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        'types' : types,\n",
    "        'confidence': '0.35',\n",
    "        \"text\":text,\n",
    "    }\n",
    "    return headers, params\n",
    "\n",
    "# return response in JSON format\n",
    "def queryDBPediaAnnotation(url,header,params):\n",
    "    try:\n",
    "        response = requests.post(url,headers=header, params=params).json()\n",
    "        \n",
    "    except Exception as ex:\n",
    "        if hasattr(ex, 'message'):\n",
    "            print(ex.message)\n",
    "        else:\n",
    "            print(ex)\n",
    "        raise Exception(ex)\n",
    "    # finally:\n",
    "        # print(response)\n",
    "\n",
    "    return response\n",
    "\n",
    "def executeQueryDbpedia(q, f='application/json'):\n",
    "    epr = \"http://dbpedia.org/sparql\"\n",
    "    try:\n",
    "        params = {'query': q}\n",
    "        resp = requests.get(epr, params=params, headers={'Accept': f})\n",
    "    #    return resp.text\n",
    "        return resp\n",
    "    except Exception as e:\n",
    "        print(e, file=sys.stdout)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa431c11-e50f-4b89-9dc6-c326fb2e5719",
   "metadata": {},
   "source": [
    "## 1. Extract entities: People, places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01d49591-ee8a-45e1-ac0d-dafbdc450ea7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "905775.csv\n",
      "90698.csv\n",
      "907187.csv\n",
      "909222.csv\n",
      "910077.csv\n",
      "916486.csv\n",
      "918090.csv\n",
      "922045.csv\n",
      "92450.csv\n",
      "926691.csv\n",
      "933559.csv\n",
      "936912.csv\n",
      "939726.csv\n",
      "948305.csv\n",
      "950070.csv\n",
      "952194.csv\n",
      "965420.csv\n",
      "966302.csv\n",
      "9700.csv\n",
      "97205.csv\n",
      "973172.csv\n",
      "975107.csv\n",
      "980182.csv\n",
      "981787.csv\n",
      "98217.csv\n",
      "984106.csv\n",
      "984155.csv\n",
      "984900.csv\n",
      "984982.csv\n",
      "98513.csv\n",
      "991714.csv\n",
      "994118.csv\n",
      "997083.csv\n"
     ]
    }
   ],
   "source": [
    "# 1. Extract People and Places entities\n",
    "entitiyTypes = ['DBpedia:Person','DBpedia:MusicalArtist','DBpedia:Place','DBpedia:SocietalEvent']\n",
    "\n",
    "# Count the number of files read to include breaks\n",
    "count = 0\n",
    "for file_name in df_files.itertuples():\n",
    "    count +=1\n",
    "    # start = time.time()\n",
    "    print(file_name.file_name)\n",
    "    # Read file with segmented sentences\n",
    "    biography_df = pd.read_csv('indexedSentences/'+file_name.file_name)\n",
    "    df_entities = pd.DataFrame()\n",
    "    \n",
    "    # for each sentence in each biography\n",
    "    for sentence_row in biography_df.itertuples():\n",
    "        ## send sentence text and return params for query\n",
    "        hdrs, prms = setDbPediaAnnotationServiceParameters(sentence_row.sentences)\n",
    "        try:\n",
    "            # obtain response using DBpedia spotlight\n",
    "            responseJSON = queryDBPediaAnnotation(urlAnnotation,hdrs,prms)\n",
    "            # UPDATE: save responses from DBP Spotlight\n",
    "            if 'Resources' in responseJSON:\n",
    "                file_exists = os.path.isfile('cacheSpotlightResponse/'+file_name.file_name)\n",
    "                df_resources = pd.DataFrame.from_dict(responseJSON['Resources'])\n",
    "                df_resources['sentence']=sentence_row.sentences\n",
    "                df_resources['sentenceIndex']=sentence_row.sentenceIndex\n",
    "                df_resources['paragraphIndex'] = sentence_row.paragraphIndex\n",
    "                df_resources['section'] = sentence_row.section\n",
    "                if not file_exists:\n",
    "                    df_resources.to_csv('cacheSpotlightResponse/'+file_name.file_name,index=False)\n",
    "                else:\n",
    "                    df_resources.to_csv('cacheSpotlightResponse/'+file_name.file_name,mode='a',index=False,header=False)\n",
    "                \n",
    "        except Exception as ex:\n",
    "            print(\"****\")\n",
    "            if hasattr(ex, 'message'):\n",
    "                print(ex.message)\n",
    "            else:\n",
    "                print(ex)\n",
    "\n",
    "        # if entities People, places, events using spotlight\n",
    "        if 'Resources' in responseJSON:\n",
    "            # parse response to a dataframe\n",
    "            df_resources = pd.DataFrame.from_dict(responseJSON['Resources'])\n",
    "            df_resources.rename(columns={'@URI':'URI','@types':'types','@surfaceForm':'surfaceForm','@support':'support','@offset':'offset','@similarityScore':'similarityScore',\n",
    "                                        '@percentageOfSecondRank':'percentageOfSecondRank'}, inplace=True)\n",
    "            \n",
    "            # filter only rows for entities with a category\n",
    "            df = df_resources[~df_resources['types'].isna()].copy()\n",
    "\n",
    "            # UPDATE: improving entity recognition\n",
    "            df_result = pd.DataFrame()\n",
    "            #\n",
    "            \n",
    "            if not df.empty:\n",
    "                #df_result = pd.DataFrame()\n",
    "                # assign the type of entity found, according to the categories\n",
    "                for entity in entitiyTypes:\n",
    "                    df_temp = df[df['types'].str.contains(entity)].copy()\n",
    "                    \n",
    "                    if not df_temp.empty:\n",
    "                        # df_temp.head(2)\n",
    "                        if entity == 'DBpedia:Person' or entity == 'DBpedia:MusicalArtist':\n",
    "                            df_temp['entType'] = 'person'\n",
    "                        elif entity == 'DBpedia:Place':\n",
    "                            df_temp['entType'] = 'place'\n",
    "                        elif entity == 'DBpedia:SocietalEvent' or entity == 'DBpedia:Event':\n",
    "                            df_temp['entType'] = 'event'\n",
    "                            \n",
    "                        df_result = df_result.append(df_temp)\n",
    "                        \n",
    "            # UPDATE:\n",
    "            # Adding process to query entities without the type\n",
    "            # filter only rows without category\n",
    "            df = df_resources.loc[df_resources['types'] == ''] \n",
    "            if not df.empty:\n",
    "                for item in df.itertuples():\n",
    "                    uri = item.URI\n",
    "                    query_text = \"SELECT * WHERE { <\" + uri + \"> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>  ?o }\"\n",
    "                    # Execute query against sparql endpoint, query types\n",
    "                    results = executeQueryDbpedia(query_text).json()\n",
    "                    \n",
    "                    # if query returns a response\n",
    "                    if 'results' in results:\n",
    "                        df_none = pd.DataFrame.from_dict(results['results']['bindings'])\n",
    "\n",
    "                        for item in df_none.itertuples():\n",
    "                            df_temp = pd.DataFrame()\n",
    "                            # find types Place and Person\n",
    "                            if 'http://dbpedia.org/ontology/Place' == item.o['value']:\n",
    "                                df_result = df_result.append({'entType':'place','URI':uri,'types':item.o['value']}, ignore_index=True)\n",
    "                            elif 'http://dbpedia.org/ontology/Person' == item.o['value']:\n",
    "                                df_result = df_result.append({'entType':'person','URI':uri,'types':item.o['value']}, ignore_index=True)\n",
    "            #\n",
    "            if not df_result.empty:\n",
    "                df_result['sentence']=sentence_row.sentences\n",
    "                df_result['sentenceIndex']=sentence_row.sentenceIndex\n",
    "                df_result['paragraphIndex'] = sentence_row.paragraphIndex\n",
    "                df_result['section'] = sentence_row.section\n",
    "                df_result.rename(columns = {'surfaceForm':'entity'},inplace = True)\n",
    "                \n",
    "                df_entities = df_entities.append(df_result)\n",
    "\n",
    "    # append time\n",
    "    df_entities['wikiPageID'] = sentence_row.wikiId\n",
    "    df_entities.to_csv('extractedEntitiesPersonPlaceOnly/'+file_name.file_name,index=False)\n",
    "    # end = time.time()\n",
    "    # print(\"The time of execution of above program is :\", end-start)\n",
    "    time.sleep(1)\n",
    "    \n",
    "    if (count % 50) == 0:\n",
    "        time.sleep(120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "aed0d4ab-7cae-46ac-805c-d6366be2f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "+ = match 1 or more\n",
    "? = match 0 or 1 repetitions.\n",
    "* = match 0 or MORE repetitions\t  \n",
    ". = Any character except a new line\n",
    "CD\tcardinal digit\n",
    "NN\tnoun, singular 'desk'\n",
    "NNS\tnoun plural\t'desks'\n",
    "NNP\tproper noun, singular\t'Harrison'\n",
    "NNPS\tproper noun, plural\t'Americans'\n",
    "IN\tpreposition/subordinating conjunction\n",
    "RB\tadverb\tvery, silently,\n",
    "RBR\tadverb, comparative\tbetter\n",
    "RBS\tadverb, superlative\tbest\n",
    "\"\"\"\n",
    "def set_pattern_time():\n",
    "    pattern = r\"\"\"DT: \n",
    "    {<CD?><NNP|CD?><CD?>} #complete dates Eg. 23 January 1983\n",
    "    {<NNP?><CD?><,?><CD?>} # December 13, 2000\n",
    "    {<CD?></><CD?></><CD?>} #complete dates 23/02/2021\n",
    "    {<CD?><-><CD?><-><CD?>} #complete dates 23-02-2021\n",
    "    HO:\n",
    "    {<CD>+<NN>+} # hour only\n",
    "    RN: \n",
    "    {<IN><CD>+<IN|TO|CC>+<CD>+} # between YYYY and <> YYYY, from 1938 to 1939\n",
    "    {<IN>+<DT>?<\\d>} # \"in XXXX\" (year)\n",
    "    <\\W?>{<CD>}<\\W?> # year in between special characters\n",
    "    {<NNP><CD>} #incomplete date January 2003, Fall 1994\n",
    "    {<NN>?<IN|DT><CD>} # years dt the 1990s, leukemia in 1996, age of 43,the 1990s\n",
    "    {<CD><IN>} # 1984 novel,1954–58\n",
    "    REF:\n",
    "    {<IN>+<NN>+<CD>+} # by age 43\n",
    "    {<NN><IN><CD>} # Eg. fall/winter of 1345, age of 43\n",
    "    \"\"\"\n",
    "    return pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5370efe2-dfde-46fa-9884-14051da03a4f",
   "metadata": {},
   "source": [
    "## 2. Extract time expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e68abe2d-9c07-4c59-a7aa-d6bf25dc4908",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10085.csv\n"
     ]
    }
   ],
   "source": [
    "pattern = set_pattern_time()\n",
    "count = 0\n",
    "for file_name in df_files.itertuples():\n",
    "    count +=1\n",
    "    # start = time.time()\n",
    "    print(file_name.file_name)\n",
    "    # Read file with segmented sentences\n",
    "    biography_df = pd.read_csv('indexedSentences/'+file_name.file_name)\n",
    "    df_time_ent = pd.DataFrame()\n",
    "    # df_entities = pd.DataFrame()\n",
    "    \n",
    "    # for each sentence in each biography\n",
    "    for sentence_row in biography_df.itertuples():\n",
    "        # now use the same sentence to analyse if a time entity is present\n",
    "        # for sentence_row in biography_df.itertuples():\n",
    "        df_temp = pd.DataFrame()\n",
    "        timeEntityList = []\n",
    "\n",
    "        tokenized_sent=nt.word_tokenize(sentence_row.sentences)\n",
    "        pos_sentences=nltk.pos_tag(tokenized_sent)\n",
    "        cp = nltk.RegexpParser(pattern)\n",
    "        cs = cp.parse(pos_sentences)\n",
    "\n",
    "        # loop to search for the POST TAGs related to TIME\n",
    "        for ne in cs:\n",
    "            res = \"\"\n",
    "            if hasattr(ne, \"label\"):\n",
    "                # print(type(ne[0:]))\n",
    "                # print(ne.label(), ne[0:])\n",
    "\n",
    "                for i in ne[0:]:\n",
    "                    res += i[0] + \" \"\n",
    "                res = res.strip()\n",
    "                # print(res)\n",
    "                    # print(t)\n",
    "                timeEntityList.append(res)\n",
    "\n",
    "        # if we have some time entities indentified\n",
    "        if timeEntityList:\n",
    "            df_temp['entity']=timeEntityList\n",
    "            df_temp['sentence']= sentence_row.sentences\n",
    "            df_temp['sentenceIndex']=sentence_row.sentenceIndex\n",
    "            df_temp['paragraphIndex'] = sentence_row.paragraphIndex\n",
    "            df_temp['section'] = sentence_row.section\n",
    "            df_temp['entType'] = 'time'\n",
    "            df_temp['wikiPageID'] = sentence_row.wikiId\n",
    "\n",
    "            df_time_ent = df_time_ent.append(df_temp)\n",
    "\n",
    "    # # append time\n",
    "    df_entities = pd.read_csv('extractedEntitiesPersonPlaceOnly/'+file_name.file_name)\n",
    "    df_entities.append(df_time_ent).to_csv('extractedEntities/'+file_name.file_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd46b25-d969-49f5-90ca-c4525b856f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "8d7bc8e2-8198-4c8a-82b4-b135d4c79df4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RN [('In', 'IN'), ('1957', 'CD')]\n",
      "In 1957\n",
      "RN [('1931–1991', 'CD')]\n",
      "1931–1991\n",
      "RN [('leukemia', 'NN'), ('in', 'IN'), ('1996', 'CD')]\n",
      "leukemia in 1996\n",
      "DT [('April', 'NNP'), ('12', 'CD'), (',', ','), ('1999', 'CD')]\n",
      "April 12 , 1999\n",
      "RN [('age', 'NN'), ('of', 'IN'), ('67', 'CD')]\n",
      "age of 67\n",
      "DT [('December', 'NNP'), ('4', 'CD'), (',', ','), ('1940', 'CD')]\n",
      "December 4 , 1940\n",
      "DT [('December', 'NNP'), ('1', 'CD'), (',', ','), ('1942', 'CD')]\n",
      "December 1 , 1942\n",
      "RN [('1954–58', 'CD')]\n",
      "1954–58\n",
      "RN [('1958–86', 'CD')]\n",
      "1958–86\n",
      "RN [('from', 'IN'), ('1941', 'CD'), ('to', 'TO'), ('1951', 'CD')]\n",
      "from 1941 to 1951\n",
      "RN [('from', 'IN'), ('1942', 'CD'), ('to', 'TO'), ('1945', 'CD')]\n",
      "from 1942 to 1945\n",
      "RN [('1947', 'CD'), ('novel', 'IN')]\n",
      "1947 novel\n",
      "RN [('Fall', 'NNP'), ('1994', 'CD')]\n",
      "Fall 1994\n",
      "HO [('One', 'CD'), ('Evening', 'NN')]\n",
      "One Evening\n",
      "RN [('1983', 'CD')]\n",
      "1983\n",
      "RN [('In', 'IN'), ('1957', 'CD')]\n",
      "In 1957\n",
      "RN [('1931–1991', 'CD')]\n",
      "1931–1991\n",
      "RN [('the', 'DT'), ('1990s', 'CD')]\n",
      "the 1990s\n"
     ]
    }
   ],
   "source": [
    "sentence = \"In 1957, Auchincloss married Adele Burden Lawrence (1931–1991), the daughter of Florence Irvin (née Burden) Lawrence and Blake Leigh Lawrence. Martin was diagnosed with leukemia in 1996, and died on April 12, 1999 in Branson, Missouri at the age of 67. He applied to join the Naval Reserve as an intelligence specialist on December 4, 1940 and was appointed as a lieutenant on December 1, 1942. After taking a break to pursue full-time writing, Auchincloss returned to working as a lawyer, first as an associate (1954–58) and then as a partner (1958–86) at Hawkins, Delafield and Wood in New York City as a wills and trusts attorney, while writing at the rate of a book a year. Auchincloss was an associate at Sullivan & Cromwell from 1941 to 1951 (with an interruption for war service from 1942 to 1945 in the United States Navy during World War II, which might have inspired his 1947 novel The Indifferent Children). SourcesGeorge Plimpton (Fall 1994). One Evening in Chicago (1983). In 1957, Auchincloss married Adele Burden Lawrence (1931–1991), the daughter of Florence Irvin (née Burden) Lawrence and Blake Leigh Lawrence towards immigrants during the 1990s and . Las majas del bergantín ('81 and '86 versions)\"\n",
    "pattern = set_pattern_time()\n",
    "tokenized_sent=nt.word_tokenize(sentence)\n",
    "pos_sentences=nltk.pos_tag(tokenized_sent)\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(pos_sentences)\n",
    "#print(cs)\n",
    "\n",
    "# loop to search for the POST TAGs related to TIME\n",
    "for ne in cs:\n",
    "    res = \"\"\n",
    "    if hasattr(ne, \"label\"):\n",
    "        #print(type(ne[0:]))\n",
    "        print(ne.label(), ne[0:])\n",
    "\n",
    "        for i in ne[0:]:\n",
    "            res += i[0] + \" \"\n",
    "        res = res.strip()\n",
    "        print(res)\n",
    "        # print(t)\n",
    "        timeEntityList.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d0698f-19fb-416e-a03c-785a1feb6f74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8662400a-c28f-46ba-95c5-49ef5a9e5dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
