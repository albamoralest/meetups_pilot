{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4921149-8064-4594-8b85-babce5c8d76a",
   "metadata": {},
   "source": [
    "### Capturing Places and Persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b77f17-c5c3-42c3-8590-a39cd73d255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This notebook processes text from music personalities' biographies and extract historical meetups information \n",
    "#### Pre-requirements:\n",
    "#### Entity annotations after executing \"02_queryDbpedia.ipynb\" Notebook\n",
    "\n",
    "#### For each file with entity annotations\n",
    "#### - Read files from cacheSpotlightResponse/\n",
    "#### - Identify People entities:\n",
    "####   - \"http://dbpedia.org/ontology/Person\"\n",
    "####   - \"http://dbpedia.org/ontology/MusicalArtist\"\n",
    "#### - Identify Place entities:\n",
    "####   - 'http://dbpedia.org/ontology/Place'\n",
    "#### - When an entity has an empty type\n",
    "####   - Read corresponding emptyEntity response from DBpedia in cacheSpotlightResponse/ directory\n",
    "#### - Store annotations in extractedEntitiesPersonPlaceOnly/\n",
    "\n",
    "#### Directories information:\n",
    "#### cacheSpotlightResponse/ : collection of biographies in CSV format. Each biography contains the list of entities identified using DBpedia Spotlight, each linked to its corresponding sentence\n",
    "#### extractedEntitiesPersonPlaceOnly/ : response from DBpedia Spotlight entity annotation grouped by biography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a0900ea-2fc1-4e27-8259-cc2b432db572",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from _datetime import date\n",
    "import time\n",
    "from operator import itemgetter\n",
    "\n",
    "# For DBpedia spotlight, PPE entities\n",
    "import requests\n",
    "import pycurl\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b04f3c-1504-4d1f-8069-7e7d31a4aec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For nltk time entities\n",
    "# time entity\n",
    "import nltk.tokenize as nt\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import Tree\n",
    "\n",
    "# if not installed\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "de1ca3a4-8c9f-459f-839d-b696f941c7bb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1002 entries, 0 to 1001\n",
      "Data columns (total 1 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   file_name  1002 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 8.0+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000228.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100273.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100487.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10085.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1009725.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     file_name\n",
       "0  1000228.csv\n",
       "1   100273.csv\n",
       "2   100487.csv\n",
       "3    10085.csv\n",
       "4  1009725.csv"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading every CSV with indexed sentences\n",
    "# return a list object of files in the given folder\n",
    "files_list = [f for f in os.listdir('indexedSentences') if not f.startswith('.')]\n",
    "# parse to dataframe\n",
    "df_files = pd.DataFrame(files_list, columns=['file_name'])\n",
    "# df_files = df_files.query(\"file_name=='10085.csv'\")\n",
    "df_files.to_csv('totalBiographiesEntities.csv',index=False)\n",
    "df_files.info()\n",
    "df_files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bfb2ab8-9aeb-434b-a401-04a73eff7b92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 0 entries\n",
      "Data columns (total 1 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   file_name  0 non-null      object\n",
      "dtypes: object(1)\n",
      "memory usage: 0.0+ bytes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [file_name]\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract only the ones that do not exist in folder\n",
    "files_list = [f for f in os.listdir('extractedEntities') if not f.startswith('.')]\n",
    "# parse to dataframe\n",
    "df_query = pd.DataFrame(files_list, columns=['file_name'])\n",
    "df_result = df_files[~df_files['file_name'].isin(df_query['file_name'])]\n",
    "df_files = df_result\n",
    "df_files.to_csv('totalBiographiesEntities.csv',index=False)\n",
    "df_files.info()\n",
    "df_files.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f05ca0-020a-467c-a8d3-8ef6c27a6d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING ONLY FOR SAMPLING - 1002\n",
    "files_list = [f for f in os.listdir('extractedEntities') if not f.startswith('.')]\n",
    "# parse to dataframe\n",
    "df_query = pd.DataFrame(files_list, columns=['file_name'])\n",
    "df_query.to_csv('testingDatasetFileNames.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfaca6f-7b2c-4078-a7b6-615e2a551cda",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Process DBpedial Spotlight entity annotation: functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8013d76-363f-4bff-b3cb-d8e152a5ed67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def executeQueryDbpedia(q, f='application/json'):\n",
    "    epr = \"http://dbpedia.org/sparql\"\n",
    "    try:\n",
    "        params = {'query': q}\n",
    "        resp = requests.get(epr, params=params, headers={'Accept': f})\n",
    "    #    return resp.text\n",
    "        return resp\n",
    "    except Exception as e:\n",
    "        # print(e, file=sys.stdout)\n",
    "        if hasattr(e, 'message'):\n",
    "            print(e.message)\n",
    "        else:\n",
    "            print(e)\n",
    "        raise\n",
    "        \n",
    "# retrieve entities information when they are not in cache\n",
    "def queryEntityLeft(uri,item):\n",
    "    # retrieve the next id to store the empty entitites\n",
    "    df_master = pd.read_csv('cacheSpotlightResponse/emptyTypes_master.csv')\n",
    "    df_master['id'] = df_master['file_name'].str.replace('emptyTypes_','')\n",
    "    df_master['id'] = df_master['id'].str.replace('.csv','')\n",
    "    df_master['id'] = df_master['id'].astype(str).astype(int)\n",
    "    df_master = df_master.sort_values(by='id', ascending=False)\n",
    "    last_file_id = df_master['id'].loc[df_master.index[0]]\n",
    "    last_file_id = int(last_file_id)+1\n",
    "    # last_file_id +=1\n",
    "    \n",
    "    df_results = pd.DataFrame()\n",
    "    query_text = \"SELECT * WHERE { <\" + uri + \"> <http://www.w3.org/1999/02/22-rdf-syntax-ns#type>  ?o }\"\n",
    "    try:\n",
    "        # Execute query against sparql endpoint, query types\n",
    "        results = executeQueryDbpedia(query_text).json()\n",
    "        # print(results)\n",
    "        # if query returns a response\n",
    "        if 'results' in results:\n",
    "            # to obtain the list of types\n",
    "            res_1 = list(map(itemgetter('o'), results['results']['bindings']))\n",
    "            res_2 = list(map(itemgetter('value'), res_1))\n",
    "\n",
    "            df_results['types'] = res_2\n",
    "            df_results['URI'] = item.URI\n",
    "            df_results['entity'] = item.entity\n",
    "            df_results['support'] = item.support\n",
    "            df_results['offset'] = item.offset\n",
    "            df_results['similarityScore'] = item.similarityScore\n",
    "            df_results['percentageOfSecondRank'] = item.percentageOfSecondRank\n",
    "\n",
    "            df_results['sentenceIndex']=item.sentenceIndex\n",
    "            df_results['paragraphIndex'] = item.paragraphIndex\n",
    "            df_results['section'] = item.section\n",
    "\n",
    "            df_new_master_row = pd.DataFrame({'URI':[item.URI],'entity':[item.entity],\n",
    "                                                          'file_name':['emptyTypes_{}.csv'.format(str(last_file_id))]})\n",
    "            df_new_master_row.to_csv('cacheSpotlightResponse/emptyTypes_master.csv',mode='a',\n",
    "                                 index=False,header=False)\n",
    "            # print(\"Saved master file: \" + str(last_file_id) + \". Len: \" + str(len(df_new_master_row)))\n",
    "\n",
    "            file_exists = os.path.isfile('cacheSpotlightResponse/emptyTypes_'+str(last_file_id)+'.csv')\n",
    "            if not file_exists:\n",
    "                df_results.to_csv('cacheSpotlightResponse/emptyTypes_'+str(last_file_id)+'.csv',index=False)\n",
    "            else:\n",
    "                df_results.to_csv('cacheSpotlightResponse/emptyTypes_'+str(last_file_id)+'.csv',mode='a',\n",
    "                                 index=False,header=False)\n",
    "    except Exception as ex:\n",
    "        print(\"Blank type: ****\")\n",
    "        if hasattr(ex, 'message'):\n",
    "            print(ex.message)\n",
    "        else:\n",
    "            print(ex)\n",
    "        # print(ex, file=sys.stdout)\n",
    "    return df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfce1a31-f732-42f5-bc72-1de5dedd9702",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 0 entries\n",
      "Data columns (total 1 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   file_name  0 non-null      object\n",
      "dtypes: object(1)\n",
      "memory usage: 0.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "# Create error file for later review\n",
    "df_error = pd.DataFrame(columns=['file_name'])\n",
    "df_error.to_csv('temp_error.csv',index=False)\n",
    "\n",
    "df_error.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa431c11-e50f-4b89-9dc6-c326fb2e5719",
   "metadata": {},
   "source": [
    "## 1. Extract entities: People, places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8ed127-0b8e-4992-ae62-1a05602a6cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkPersonLinkedEntity():\n",
    "    object_list = [\"rdf:type dbo:PersonFunction\",\"dct:subject dbc:Musical_terminology\"]\n",
    "    # iterate over all the person entities found\n",
    "    for entity_row in entityList_df.itertuples():\n",
    "        role = False\n",
    "        for item in object_list:\n",
    "            # build query\n",
    "            query_text = \"ASK  { <{}> {}}\".format(entity_row.URI)\n",
    "            #read cache\n",
    "            \n",
    "            # check if already in cache\n",
    "            # if in cache and response is true:\n",
    "            # role = True\n",
    "            # break\n",
    "            # else:\n",
    "            # query DBpedia\n",
    "            response = executeQueryDbpedia(query_text)\n",
    "            if response == 'true':\n",
    "                role = True\n",
    "                break\n",
    "        if role:\n",
    "            # delete the entity from the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "916af700-0240-4c0b-949c-3210d0d67b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1035724.csv\n",
      "Error: 1035724.csv\n",
      "Error tokenizing data. C error: Expected 7 fields in line 126, saw 11\n",
      "\n",
      "1043762.csv\n",
      "Error: 1043762.csv\n",
      "Error tokenizing data. C error: Expected 7 fields in line 104, saw 11\n",
      "\n",
      "1047779.csv\n",
      "Error: 1047779.csv\n",
      "Error tokenizing data. C error: Expected 7 fields in line 106, saw 11\n",
      "\n",
      "1048151.csv\n",
      "Error: 1048151.csv\n",
      "Error tokenizing data. C error: Expected 7 fields in line 310, saw 11\n",
      "\n",
      "1048172.csv\n",
      "Error: 1048172.csv\n",
      "Error tokenizing data. C error: Expected 7 fields in line 190, saw 11\n",
      "\n",
      "1049483.csv\n",
      "Error: 1049483.csv\n",
      "Error tokenizing data. C error: Expected 7 fields in line 393, saw 11\n",
      "\n",
      "1052490.csv\n",
      "Error: 1052490.csv\n",
      "Error tokenizing data. C error: Expected 7 fields in line 673, saw 11\n",
      "\n",
      "1056463.csv\n",
      "Error: 1056463.csv\n",
      "Error tokenizing data. C error: Expected 7 fields in line 589, saw 11\n",
      "\n",
      "105767.csv\n",
      "Error: 105767.csv\n",
      "Error tokenizing data. C error: Expected 7 fields in line 357, saw 11\n",
      "\n",
      "1058567.csv\n",
      "Error: 1058567.csv\n",
      "Error tokenizing data. C error: Expected 7 fields in line 161, saw 11\n",
      "\n",
      "1059399.csv\n",
      "Error: 1059399.csv\n",
      "Error tokenizing data. C error: Expected 7 fields in line 155, saw 11\n",
      "\n",
      "106366.csv\n",
      "Error: 106366.csv\n",
      "Error tokenizing data. C error: Expected 7 fields in line 415, saw 11\n",
      "\n",
      "10671.csv\n",
      "Error: 10671.csv\n",
      "'types'\n",
      "1068160.csv\n",
      "Error: 1068160.csv\n",
      "'types'\n",
      "1070521.csv\n",
      "Error: 1070521.csv\n",
      "'types'\n",
      "1073691.csv\n",
      "Error: 1073691.csv\n",
      "'types'\n",
      "1077508.csv\n",
      "Error: 1077508.csv\n",
      "'types'\n",
      "1081839.csv\n",
      "Error: 1081839.csv\n",
      "'types'\n",
      "1084179.csv\n",
      "Error: 1084179.csv\n",
      "'types'\n",
      "1089533.csv\n",
      "Error: 1089533.csv\n",
      "'types'\n",
      "1092607.csv\n",
      "Error: 1092607.csv\n",
      "'types'\n",
      "1093129.csv\n",
      "Error: 1093129.csv\n",
      "'types'\n",
      "1097923.csv\n",
      "Error: 1097923.csv\n",
      "'types'\n",
      "1098118.csv\n",
      "Error: 1098118.csv\n",
      "'types'\n",
      "1103482.csv\n",
      "Error: 1103482.csv\n",
      "'types'\n",
      "1107893.csv\n",
      "Error: 1107893.csv\n",
      "'types'\n",
      "1111730.csv\n",
      "Error: 1111730.csv\n",
      "'types'\n",
      "1113259.csv\n",
      "Error: 1113259.csv\n",
      "'types'\n",
      "1113588.csv\n",
      "Error: 1113588.csv\n",
      "'types'\n",
      "1115155.csv\n",
      "Error: 1115155.csv\n",
      "'types'\n",
      "1118112.csv\n",
      "Error: 1118112.csv\n",
      "'types'\n",
      "1127222.csv\n",
      "Error: 1127222.csv\n",
      "'types'\n",
      "1129635.csv\n",
      "Error: 1129635.csv\n",
      "'types'\n",
      "113049.csv\n",
      "Error: 113049.csv\n",
      "'types'\n",
      "113560.csv\n",
      "Error: 113560.csv\n",
      "'types'\n",
      "1142145.csv\n",
      "Error: 1142145.csv\n",
      "'types'\n",
      "1147577.csv\n",
      "Error: 1147577.csv\n",
      "'types'\n",
      "1148248.csv\n",
      "Error: 1148248.csv\n",
      "'types'\n",
      "1150533.csv\n",
      "Error: 1150533.csv\n",
      "'types'\n",
      "1151374.csv\n",
      "Error: 1151374.csv\n",
      "'types'\n",
      "1153177.csv\n",
      "Error: 1153177.csv\n",
      "'types'\n",
      "1164879.csv\n",
      "Error: 1164879.csv\n",
      "'types'\n",
      "1174545.csv\n",
      "Error: 1174545.csv\n",
      "'types'\n",
      "1174834.csv\n",
      "Error: 1174834.csv\n",
      "'types'\n",
      "1175222.csv\n",
      "Error: 1175222.csv\n",
      "'types'\n",
      "1178548.csv\n",
      "Error: 1178548.csv\n",
      "'types'\n",
      "1181295.csv\n",
      "Error: 1181295.csv\n",
      "'types'\n",
      "1181499.csv\n",
      "Error: 1181499.csv\n",
      "'types'\n",
      "1186480.csv\n",
      "Error: 1186480.csv\n",
      "'types'\n",
      "1188821.csv\n",
      "Error: 1188821.csv\n",
      "'types'\n",
      "1189627.csv\n",
      "Error: 1189627.csv\n",
      "'types'\n",
      "1194878.csv\n",
      "Error: 1194878.csv\n",
      "'types'\n",
      "1195717.csv\n",
      "Error: 1195717.csv\n",
      "'types'\n",
      "1196793.csv\n",
      "Error: 1196793.csv\n",
      "'types'\n",
      "1205991.csv\n",
      "Error: 1205991.csv\n",
      "'types'\n",
      "1209484.csv\n",
      "Error: 1209484.csv\n",
      "'types'\n",
      "1209685.csv\n",
      "Error: 1209685.csv\n",
      "'types'\n",
      "1213916.csv\n",
      "Error: 1213916.csv\n",
      "'types'\n",
      "1215145.csv\n",
      "Error: 1215145.csv\n",
      "'types'\n",
      "1228021.csv\n",
      "Error: 1228021.csv\n",
      "'types'\n",
      "1232492.csv\n",
      "Error: 1232492.csv\n",
      "'types'\n",
      "1234422.csv\n",
      "Error: 1234422.csv\n",
      "'types'\n",
      "1234606.csv\n",
      "Error: 1234606.csv\n",
      "'types'\n",
      "1236563.csv\n",
      "Error: 1236563.csv\n",
      "'types'\n",
      "1239372.csv\n",
      "Error: 1239372.csv\n",
      "'types'\n",
      "1245489.csv\n",
      "Error: 1245489.csv\n",
      "'types'\n",
      "125311.csv\n",
      "Error: 125311.csv\n",
      "'types'\n",
      "1253783.csv\n",
      "Error: 1253783.csv\n",
      "'types'\n",
      "1253793.csv\n",
      "Error: 1253793.csv\n",
      "'types'\n",
      "1254002.csv\n",
      "Error: 1254002.csv\n",
      "'types'\n",
      "1254005.csv\n",
      "Error: 1254005.csv\n",
      "'types'\n",
      "1261779.csv\n",
      "Error: 1261779.csv\n",
      "'types'\n",
      "1265845.csv\n",
      "Error: 1265845.csv\n",
      "'types'\n",
      "1271437.csv\n",
      "Error: 1271437.csv\n",
      "'types'\n",
      "1271796.csv\n",
      "Error: 1271796.csv\n",
      "'types'\n",
      "1272132.csv\n",
      "Error: 1272132.csv\n",
      "'types'\n",
      "1275454.csv\n",
      "Error: 1275454.csv\n",
      "'types'\n",
      "1277042.csv\n",
      "Error: 1277042.csv\n",
      "'types'\n",
      "1285387.csv\n",
      "Error: 1285387.csv\n",
      "'types'\n",
      "1291198.csv\n",
      "Error: 1291198.csv\n",
      "'types'\n",
      "1291515.csv\n",
      "Error: 1291515.csv\n",
      "'types'\n",
      "129194.csv\n",
      "Error: 129194.csv\n",
      "'types'\n",
      "12945.csv\n",
      "Error: 12945.csv\n",
      "'types'\n",
      "1296456.csv\n",
      "Error: 1296456.csv\n",
      "'types'\n",
      "1302505.csv\n",
      "Error: 1302505.csv\n",
      "'types'\n",
      "1303365.csv\n",
      "Error: 1303365.csv\n",
      "'types'\n",
      "1310858.csv\n",
      "Error: 1310858.csv\n",
      "'types'\n",
      "1320240.csv\n",
      "Error: 1320240.csv\n",
      "'types'\n",
      "1321577.csv\n",
      "Error: 1321577.csv\n",
      "'types'\n",
      "1322881.csv\n",
      "Error: 1322881.csv\n",
      "'types'\n",
      "1323952.csv\n",
      "Error: 1323952.csv\n",
      "'types'\n",
      "1324135.csv\n",
      "Error: 1324135.csv\n",
      "'types'\n",
      "1338168.csv\n",
      "Error: 1338168.csv\n",
      "'types'\n",
      "1338619.csv\n",
      "Error: 1338619.csv\n",
      "'types'\n",
      "1341923.csv\n",
      "Error: 1341923.csv\n",
      "'types'\n",
      "1343880.csv\n",
      "Error: 1343880.csv\n",
      "'types'\n",
      "1346743.csv\n",
      "Error: 1346743.csv\n",
      "'types'\n",
      "1351998.csv\n",
      "Error: 1351998.csv\n",
      "'types'\n",
      "1354212.csv\n",
      "Error: 1354212.csv\n",
      "'types'\n",
      "1354646.csv\n",
      "Error: 1354646.csv\n",
      "'types'\n",
      "1354841.csv\n",
      "Error: 1354841.csv\n",
      "'types'\n",
      "1354864.csv\n",
      "Error: 1354864.csv\n",
      "'types'\n",
      "1359293.csv\n",
      "Error: 1359293.csv\n",
      "'types'\n",
      "1359335.csv\n",
      "Error: 1359335.csv\n",
      "'types'\n",
      "137869.csv\n",
      "Error: 137869.csv\n",
      "'types'\n",
      "1384081.csv\n",
      "Error: 1384081.csv\n",
      "'types'\n",
      "9548194.csv\n",
      "0\n",
      "9549837.csv\n",
      "0\n",
      "955264.csv\n",
      "0\n",
      "9555416.csv\n",
      "0\n",
      "9555676.csv\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_514/2908078643.py:20: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_master['id'] = df_master['id'].str.replace('.csv','')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9555834.csv\n",
      "1\n",
      "9559305.csv\n",
      "1\n",
      "9559601.csv\n",
      "3\n",
      "956078.csv\n",
      "2\n",
      "956082.csv\n",
      "3\n",
      "956147.csv\n",
      "2\n",
      "956166.csv\n",
      "1\n",
      "956338.csv\n",
      "1\n",
      "956432.csv\n",
      "1\n",
      "9565227.csv\n",
      "0\n",
      "956561.csv\n",
      "1\n",
      "9565871.csv\n",
      "0\n",
      "9566528.csv\n",
      "0\n",
      "9566772.csv\n",
      "4\n",
      "9572609.csv\n",
      "0\n",
      "957280.csv\n",
      "0\n",
      "957400.csv\n",
      "0\n",
      "9574007.csv\n",
      "0\n",
      "9577719.csv\n",
      "2\n",
      "9578639.csv\n",
      "0\n",
      "9579930.csv\n",
      "0\n",
      "9581533.csv\n",
      "2\n",
      "9582754.csv\n",
      "0\n",
      "9582791.csv\n",
      "0\n",
      "9583516.csv\n",
      "0\n",
      "95849.csv\n",
      "6\n",
      "9584945.csv\n",
      "1\n",
      "9585107.csv\n",
      "1\n",
      "958564.csv\n",
      "1\n",
      "9587658.csv\n",
      "0\n",
      "9587975.csv\n",
      "3\n",
      "9588366.csv\n",
      "0\n",
      "9588919.csv\n",
      "0\n",
      "9589287.csv\n",
      "1\n",
      "9589996.csv\n",
      "1\n",
      "9590626.csv\n",
      "6\n",
      "959183.csv\n",
      "1\n",
      "9592193.csv\n",
      "0\n",
      "9592833.csv\n",
      "3\n",
      "959309.csv\n",
      "28\n",
      "959635.csv\n",
      "0\n",
      "9596961.csv\n",
      "0\n",
      "9597238.csv\n",
      "1\n",
      "9598149.csv\n",
      "2\n",
      "959907.csv\n",
      "1\n",
      "9606071.csv\n",
      "5\n",
      "960698.csv\n",
      "0\n",
      "960719.csv\n",
      "2\n",
      "960995.csv\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_514/190424490.py:77: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  df_empty_type = df_empty_type[df_empty_type['URI'].str.contains(item_empty.URI)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9610231.csv\n",
      "1\n",
      "9610507.csv\n",
      "0\n",
      "9611501.csv\n",
      "0\n",
      "9611687.csv\n",
      "0\n",
      "9614249.csv\n",
      "1\n",
      "9615793.csv\n",
      "0\n",
      "961610.csv\n",
      "1\n",
      "9619552.csv\n",
      "0\n",
      "962005.csv\n",
      "1\n",
      "9620823.csv\n",
      "0\n",
      "9622272.csv\n",
      "0\n",
      "9624017.csv\n",
      "2\n",
      "9625304.csv\n",
      "0\n",
      "9625982.csv\n",
      "0\n",
      "962932.csv\n",
      "0\n",
      "962945.csv\n",
      "8\n",
      "9630296.csv\n",
      "2\n",
      "9630314.csv\n",
      "0\n",
      "9632534.csv\n",
      "4\n",
      "9632659.csv\n",
      "0\n",
      "9636031.csv\n",
      "0\n",
      "963637.csv\n",
      "2\n",
      "9637582.csv\n",
      "0\n",
      "9637998.csv\n",
      "0\n",
      "9639026.csv\n",
      "1\n",
      "963936.csv\n",
      "4\n",
      "964053.csv\n",
      "4\n",
      "9640637.csv\n",
      "1\n",
      "9640956.csv\n",
      "1\n",
      "9641694.csv\n",
      "0\n",
      "9642079.csv\n",
      "0\n",
      "964258.csv\n",
      "2\n",
      "9643503.csv\n",
      "0\n",
      "9647782.csv\n",
      "1\n",
      "9648733.csv\n",
      "0\n",
      "9650157.csv\n",
      "0\n",
      "965046.csv\n",
      "0\n",
      "9650786.csv\n",
      "0\n",
      "9650856.csv\n",
      "0\n",
      "965107.csv\n",
      "1\n",
      "965109.csv\n",
      "1\n",
      "9651675.csv\n",
      "0\n",
      "965420.csv\n",
      "0\n",
      "9654804.csv\n",
      "1\n",
      "9656399.csv\n",
      "2\n",
      "965761.csv\n",
      "0\n",
      "965824.csv\n",
      "4\n",
      "965919.csv\n",
      "0\n",
      "965956.csv\n",
      "0\n",
      "965995.csv\n",
      "1\n",
      "966302.csv\n",
      "5\n",
      "966347.csv\n",
      "0\n",
      "966488.csv\n",
      "0\n",
      "9665678.csv\n",
      "2\n",
      "9669378.csv\n",
      "2\n",
      "9669951.csv\n",
      "0\n",
      "967009.csv\n",
      "1\n",
      "96701.csv\n",
      "2\n",
      "9670175.csv\n",
      "0\n",
      "9671048.csv\n",
      "0\n",
      "9671420.csv\n",
      "0\n",
      "9672429.csv\n",
      "2\n",
      "967453.csv\n",
      "1\n",
      "968302.csv\n",
      "3\n",
      "968346.csv\n",
      "3\n",
      "9685734.csv\n",
      "0\n",
      "968617.csv\n",
      "0\n",
      "9688889.csv\n",
      "1\n",
      "9689709.csv\n",
      "0\n",
      "969048.csv\n",
      "0\n",
      "969120.csv\n",
      "1\n",
      "969231.csv\n",
      "0\n",
      "9692702.csv\n",
      "0\n",
      "9692767.csv\n",
      "0\n",
      "969645.csv\n",
      "2\n",
      "969745.csv\n",
      "0\n",
      "9697537.csv\n",
      "0\n",
      "9697717.csv\n",
      "0\n",
      "9699996.csv\n",
      "1\n",
      "9700.csv\n",
      "5\n",
      "9703782.csv\n",
      "2\n",
      "970536.csv\n",
      "1\n",
      "9708375.csv\n",
      "0\n",
      "9708854.csv\n",
      "0\n",
      "9711198.csv\n",
      "1\n",
      "9711625.csv\n",
      "1\n",
      "9712745.csv\n",
      "0\n",
      "971325.csv\n",
      "1\n",
      "9714369.csv\n",
      "0\n",
      "971561.csv\n",
      "3\n",
      "971563.csv\n",
      "0\n",
      "971622.csv\n",
      "3\n",
      "9716580.csv\n",
      "1\n",
      "9717213.csv\n",
      "1\n",
      "9719065.csv\n",
      "0\n",
      "972039.csv\n",
      "1\n",
      "97205.csv\n",
      "3\n",
      "97206.csv\n",
      "0\n",
      "972087.csv\n",
      "5\n",
      "9726259.csv\n",
      "0\n",
      "9726633.csv\n",
      "1\n",
      "972808.csv\n",
      "0\n",
      "9730256.csv\n",
      "0\n",
      "9730324.csv\n",
      "0\n",
      "973172.csv\n",
      "2\n",
      "9734523.csv\n",
      "2\n",
      "9735024.csv\n",
      "3\n",
      "9735302.csv\n",
      "0\n",
      "9736304.csv\n",
      "0\n",
      "973794.csv\n",
      "2\n",
      "974220.csv\n",
      "1\n",
      "9745146.csv\n",
      "0\n",
      "9745437.csv\n",
      "0\n",
      "974608.csv\n",
      "3\n",
      "974689.csv\n",
      "0\n",
      "9747231.csv\n",
      "0\n",
      "974917.csv\n",
      "5\n",
      "9749828.csv\n",
      "2\n",
      "975107.csv\n",
      "2\n",
      "975120.csv\n",
      "1\n",
      "9752460.csv\n",
      "0\n",
      "975392.csv\n",
      "0\n",
      "9754366.csv\n",
      "1\n",
      "9755955.csv\n",
      "1\n",
      "975711.csv\n",
      "0\n",
      "97582.csv\n",
      "7\n",
      "975918.csv\n",
      "0\n",
      "97593.csv\n",
      "1\n",
      "97602.csv\n",
      "0\n",
      "97606.csv\n",
      "0\n",
      "97641.csv\n",
      "0\n",
      "9764147.csv\n",
      "0\n",
      "9764395.csv\n",
      "0\n",
      "9769464.csv\n",
      "2\n",
      "9769681.csv\n",
      "0\n",
      "9770753.csv\n",
      "0\n",
      "9772040.csv\n",
      "1\n",
      "977271.csv\n",
      "1\n",
      "9773845.csv\n",
      "1\n",
      "9775638.csv\n",
      "0\n",
      "977741.csv\n",
      "1\n",
      "97778.csv\n",
      "4\n",
      "9778238.csv\n",
      "1\n",
      "977849.csv\n",
      "1\n",
      "9781165.csv\n",
      "0\n",
      "9781312.csv\n",
      "2\n",
      "97817.csv\n",
      "46\n",
      "9784258.csv\n",
      "0\n",
      "9785341.csv\n",
      "0\n",
      "978583.csv\n",
      "0\n",
      "978819.csv\n",
      "0\n",
      "978836.csv\n",
      "0\n",
      "9788396.csv\n",
      "0\n",
      "978989.csv\n",
      "1\n",
      "979007.csv\n",
      "3\n",
      "9790617.csv\n",
      "0\n",
      "97907.csv\n",
      "1\n",
      "979072.csv\n",
      "0\n",
      "9791346.csv\n",
      "0\n",
      "979215.csv\n",
      "1\n",
      "9792760.csv\n",
      "1\n",
      "9793876.csv\n",
      "0\n",
      "9797185.csv\n",
      "0\n",
      "9798829.csv\n",
      "0\n",
      "979884.csv\n",
      "0\n",
      "979900.csv\n",
      "1\n",
      "9800720.csv\n",
      "1\n",
      "980182.csv\n",
      "0\n",
      "98033.csv\n",
      "0\n",
      "98039.csv\n",
      "11\n",
      "98043.csv\n",
      "2\n",
      "980491.csv\n",
      "2\n",
      "9807977.csv\n",
      "0\n",
      "9808018.csv\n",
      "0\n",
      "9808442.csv\n",
      "2\n",
      "9810323.csv\n",
      "0\n",
      "9811182.csv\n",
      "0\n",
      "981210.csv\n",
      "1\n",
      "9813762.csv\n",
      "0\n",
      "981441.csv\n",
      "1\n",
      "981442.csv\n",
      "5\n",
      "9815034.csv\n",
      "0\n",
      "981787.csv\n",
      "0\n",
      "981902.csv\n",
      "0\n",
      "981974.csv\n",
      "3\n",
      "982081.csv\n",
      "2\n",
      "98217.csv\n",
      "5\n",
      "9823098.csv\n",
      "0\n",
      "982395.csv\n",
      "1\n",
      "9824.csv\n",
      "13\n",
      "9826118.csv\n",
      "0\n",
      "982692.csv\n",
      "2\n",
      "9826927.csv\n",
      "0\n",
      "9829201.csv\n",
      "0\n",
      "9830661.csv\n",
      "0\n",
      "9830772.csv\n",
      "0\n",
      "983237.csv\n",
      "2\n",
      "983395.csv\n",
      "0\n",
      "9836234.csv\n",
      "0\n",
      "983632.csv\n",
      "3\n",
      "9837267.csv\n",
      "1\n",
      "983751.csv\n",
      "0\n",
      "9839037.csv\n",
      "0\n",
      "984106.csv\n",
      "2\n",
      "984110.csv\n",
      "0\n",
      "984145.csv\n",
      "0\n",
      "984155.csv\n",
      "3\n",
      "98419.csv\n",
      "6\n",
      "9842315.csv\n",
      "2\n",
      "984464.csv\n",
      "0\n",
      "9845808.csv\n",
      "1\n",
      "984618.csv\n",
      "0\n",
      "9847218.csv\n",
      "2\n",
      "984900.csv\n",
      "0\n",
      "9849819.csv\n",
      "2\n",
      "984982.csv\n",
      "0\n",
      "98513.csv\n",
      "4\n",
      "985605.csv\n",
      "3\n",
      "9858858.csv\n",
      "2\n",
      "9859590.csv\n",
      "1\n",
      "9859631.csv\n",
      "2\n",
      "9861246.csv\n",
      "0\n",
      "986368.csv\n",
      "2\n",
      "986791.csv\n",
      "0\n",
      "9869811.csv\n",
      "0\n",
      "9870616.csv\n",
      "1\n",
      "9872066.csv\n",
      "10\n",
      "9873065.csv\n",
      "0\n",
      "9876823.csv\n",
      "0\n",
      "987722.csv\n",
      "1\n",
      "9877859.csv\n",
      "0\n",
      "98784.csv\n",
      "11\n",
      "987948.csv\n",
      "1\n",
      "9880325.csv\n",
      "1\n",
      "988041.csv\n",
      "0\n",
      "9884032.csv\n",
      "0\n",
      "988535.csv\n",
      "7\n",
      "9886236.csv\n",
      "4\n",
      "9887551.csv\n",
      "1\n",
      "988757.csv\n",
      "0\n",
      "9887955.csv\n",
      "1\n",
      "9888835.csv\n",
      "0\n",
      "988977.csv\n",
      "0\n",
      "989079.csv\n",
      "1\n",
      "9891070.csv\n",
      "0\n",
      "9891449.csv\n",
      "2\n",
      "9892674.csv\n",
      "2\n",
      "9893725.csv\n",
      "0\n",
      "9894755.csv\n",
      "0\n",
      "989955.csv\n",
      "0\n",
      "99033.csv\n",
      "5\n",
      "9903395.csv\n",
      "0\n",
      "990446.csv\n",
      "1\n",
      "990631.csv\n",
      "3\n",
      "9907278.csv\n",
      "4\n",
      "990784.csv\n",
      "0\n",
      "9908202.csv\n",
      "1\n",
      "990970.csv\n",
      "7\n",
      "991041.csv\n",
      "2\n",
      "991086.csv\n",
      "3\n",
      "9911310.csv\n",
      "0\n",
      "991143.csv\n",
      "2\n",
      "9913266.csv\n",
      "0\n",
      "9915312.csv\n",
      "2\n",
      "991637.csv\n",
      "0\n",
      "991714.csv\n",
      "0\n",
      "9918787.csv\n",
      "0\n",
      "9919077.csv\n",
      "0\n",
      "991969.csv\n",
      "0\n",
      "991985.csv\n",
      "3\n",
      "992036.csv\n",
      "2\n",
      "992164.csv\n",
      "2\n",
      "9921965.csv\n",
      "0\n",
      "9922752.csv\n",
      "4\n",
      "9924969.csv\n",
      "1\n",
      "9925025.csv\n",
      "0\n",
      "992582.csv\n",
      "0\n",
      "9926126.csv\n",
      "0\n",
      "9926754.csv\n",
      "0\n",
      "9926881.csv\n",
      "0\n",
      "992696.csv\n",
      "0\n",
      "992783.csv\n",
      "9\n",
      "9930020.csv\n",
      "0\n",
      "9932536.csv\n",
      "1\n",
      "993267.csv\n",
      "0\n",
      "993314.csv\n",
      "1\n",
      "993388.csv\n",
      "0\n",
      "993393.csv\n",
      "1\n",
      "9934036.csv\n",
      "0\n",
      "993411.csv\n",
      "0\n",
      "993424.csv\n",
      "0\n",
      "9934519.csv\n",
      "1\n",
      "9934866.csv\n",
      "0\n",
      "9936199.csv\n",
      "0\n",
      "9936819.csv\n",
      "1\n",
      "993742.csv\n",
      "1\n",
      "993760.csv\n",
      "0\n",
      "9940022.csv\n",
      "0\n",
      "994021.csv\n",
      "2\n",
      "994062.csv\n",
      "0\n",
      "994095.csv\n",
      "1\n",
      "994118.csv\n",
      "0\n",
      "99422.csv\n",
      "1\n",
      "9944062.csv\n",
      "0\n",
      "994409.csv\n",
      "0\n",
      "994552.csv\n",
      "4\n",
      "9951220.csv\n",
      "0\n",
      "9952115.csv\n",
      "1\n",
      "9952577.csv\n",
      "2\n",
      "9952841.csv\n",
      "1\n",
      "995412.csv\n",
      "0\n",
      "995445.csv\n",
      "3\n",
      "9955216.csv\n",
      "0\n",
      "995745.csv\n",
      "1\n",
      "996033.csv\n",
      "0\n",
      "9961377.csv\n",
      "0\n",
      "996187.csv\n",
      "4\n",
      "99636.csv\n",
      "25\n",
      "9966508.csv\n",
      "1\n",
      "9967063.csv\n",
      "0\n",
      "996823.csv\n",
      "1\n",
      "9969266.csv\n",
      "0\n",
      "997050.csv\n",
      "1\n",
      "997083.csv\n",
      "2\n",
      "99710.csv\n",
      "0\n",
      "9972228.csv\n",
      "0\n",
      "997580.csv\n",
      "0\n",
      "9978126.csv\n",
      "0\n",
      "9978828.csv\n",
      "1\n",
      "998068.csv\n",
      "0\n",
      "9982217.csv\n",
      "1\n",
      "9982878.csv\n",
      "1\n",
      "998327.csv\n",
      "2\n",
      "9984016.csv\n",
      "1\n",
      "99858.csv\n",
      "1\n",
      "99859.csv\n",
      "2\n",
      "998900.csv\n",
      "0\n",
      "998912.csv\n",
      "0\n",
      "999202.csv\n",
      "0\n",
      "9994743.csv\n",
      "0\n",
      "9996822.csv\n",
      "0\n",
      "9998731.csv\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# 1. Extract People and Places entities\n",
    "entitiyTypes = ['DBpedia:Person','DBpedia:MusicalArtist','DBpedia:Place','DBpedia:SocietalEvent']\n",
    "# http://dbpedia.org/ontology/Person\n",
    "df_entityType = pd.DataFrame({'id':[1,2],'types':['http://dbpedia.org/ontology/Person',\n",
    "                                                      'http://dbpedia.org/ontology/MusicalArtist']})\n",
    "# read master with index of entities\n",
    "df_master_cache = pd.read_csv('cacheSpotlightResponse/emptyTypes_master.csv')\n",
    "\n",
    "# use chunk to load a small number of files in memory\n",
    "for chunk in pd.read_csv('totalBiographiesEntities.csv', chunksize=30):\n",
    "    df_files = pd.DataFrame()\n",
    "    df_files['file_name'] = chunk['file_name']\n",
    "    \n",
    "    for file_name_item in df_files.itertuples():\n",
    "        try:\n",
    "            # start = time.time()\n",
    "            print(file_name_item.file_name)\n",
    "            # check if the file exists\n",
    "            file_exists = os.path.isfile('cacheSpotlightResponse/'+file_name_item.file_name)\n",
    "\n",
    "            if file_exists:\n",
    "                # read the cached results from the query\n",
    "                df = pd.read_csv('cacheSpotlightResponse/'+file_name_item.file_name)\n",
    "                df.rename(columns = {'surfaceForm':'entity'},inplace = True)\n",
    "                # select all that are not NA\n",
    "                df_ne = df.loc[~df['types'].isna()] \n",
    "\n",
    "                df_result = pd.DataFrame()\n",
    "                if len(df_ne) >0:\n",
    "                    for entity in entitiyTypes:\n",
    "                        df_temp = df_ne[df_ne['types'].str.contains(entity)].copy()\n",
    "\n",
    "                        if not df_temp.empty:\n",
    "                            if entity == 'DBpedia:Person' or entity == 'DBpedia:MusicalArtist':\n",
    "                                df_temp['entType'] = 'person'\n",
    "                            elif entity == 'DBpedia:Place':\n",
    "                                df_temp['entType'] = 'place'\n",
    "                            elif entity == 'DBpedia:SocietalEvent' or entity == 'DBpedia:Event':\n",
    "                                df_temp['entType'] = 'event'\n",
    "\n",
    "                            df_result = df_result.append(df_temp)\n",
    "\n",
    "                # now filter all the entities that are empty\n",
    "                df_e = df.loc[df['types'].isna()] \n",
    "\n",
    "                if not df_e.empty:\n",
    "                    # list of entities and the name of the files that store the results from the query\n",
    "                    df_merge = df_master_cache.merge(df_e, on=['URI','entity'],how='right')\n",
    "                    # df_merge.to_csv('extractedEntitiesPersonPlaceOnly/df_merge.csv',index=False)\n",
    "\n",
    "                    # for all documents not in cache\n",
    "                    df_cache = df_merge.loc[df_merge['file_name'].isna()]\n",
    "                    print(len(df_cache))\n",
    "                    if len(df_cache)>0:\n",
    "                        for item_empty in df_cache.itertuples():\n",
    "                            df_temp = queryEntityLeft(item_empty.URI,item_empty)\n",
    "                        df_master_cache = pd.read_csv('cacheSpotlightResponse/emptyTypes_master.csv')\n",
    "                        df_merge = df_master_cache.merge(df_e, on=['URI','entity'],how='right')\n",
    "                    # df_merge.to_csv('extractedEntitiesPersonPlaceOnly/df_merge.csv',index=False)\n",
    "\n",
    "                    df_cache = df_merge.loc[~df_merge['file_name'].isna()] \n",
    "                    for item_empty in df_cache.itertuples():\n",
    "                        # print(item_empty.entity)\n",
    "                        appendRow = False\n",
    "                        typeEntity = ''\n",
    "\n",
    "                        # read cache file\n",
    "                        file_exists = os.path.isfile('cacheSpotlightResponse/'+item_empty.file_name)\n",
    "                        if file_exists:\n",
    "                            df_empty_type = pd.read_csv('cacheSpotlightResponse/'+item_empty.file_name)\n",
    "                            df_empty_type.drop_duplicates(subset=['entity','URI','types'],keep='first')\n",
    "                            try:\n",
    "                                df_empty_type = df_empty_type.query(\"\"\"entity == \"{}\" and URI==\"{}\" \"\"\".format(item_empty.entity,item_empty.URI))\n",
    "                            except SyntaxError as ex:\n",
    "                                # df_empty_type = df_empty_type.query(\"\"\"entity == '{}' and URI=='{}'\"\"\".format(item_empty.entity,item_empty.URI))\n",
    "                                df_empty_type = df_empty_type[df_empty_type['URI'].str.contains(item_empty.URI)]\n",
    "                            except TokenError as te:\n",
    "                                df_empty_type = df_empty_type.query(\"entity == '{}' and URI=='{}'\".format(item_empty.entity,item_empty.URI))\n",
    "\n",
    "                            # find types Place\n",
    "                            df_temp = df_empty_type[df_empty_type['types'].str.contains('http://dbpedia.org/ontology/Place')].copy()\n",
    "                            if not df_temp.empty:\n",
    "                                types = df_temp['types'].loc[df_temp.index[0]]\n",
    "                                entType = 'place'\n",
    "                                appendRow = True\n",
    "                            df_temp = df_empty_type.query('types == \"http://dbpedia.org/ontology/Person\" or types==\"http://dbpedia.org/ontology/MusicalArtist\"')\n",
    "                            df_temp = df_temp.sort_values(by='types', ascending=False)\n",
    "                            if not df_temp.empty:\n",
    "                                types = df_temp['types'].loc[df_temp.index[0]]\n",
    "                                entType = 'person'\n",
    "                                appendRow = True\n",
    "                            # else:\n",
    "                            #     print(item_empty.URI)\n",
    "                            #     print(\"Empty: \" + item_empty.file_name)\n",
    "\n",
    "                            if appendRow:\n",
    "                                df_new_row = pd.DataFrame([[item_empty.URI, item_empty.support, types, item_empty.entity, item_empty.offset, item_empty.similarityScore,\n",
    "                                                          item_empty.percentageOfSecondRank, item_empty.sentence, item_empty.sentenceIndex, item_empty.paragraphIndex, \n",
    "                                                           item_empty.section]],columns=list(df))\n",
    "                                df_new_row['entType'] = entType\n",
    "                                df_result = df_result.append(df_new_row,ignore_index=True)\n",
    "                        else:\n",
    "                            print(df_merge.file_name)\n",
    "                            print(df_merge.URI)\n",
    "                            break;\n",
    "\n",
    "                    # print('No empty types')\n",
    "            # IF file does not exists, then the \n",
    "\n",
    "            df_result['wikiPageID'] = file_name_item.file_name.replace('.csv','')\n",
    "            df_result.to_csv('extractedEntitiesPersonPlaceOnly/'+file_name_item.file_name,index=False)\n",
    "            # end = time.time()\n",
    "            # print(\"The time of execution of above program is :\", end-start)\n",
    "        except (pd.errors.ParserError, KeyError) as pe:\n",
    "            print('Error: ' + file_name_item.file_name)\n",
    "            if hasattr(pe, 'message'):\n",
    "                print(pe.message)\n",
    "            else:\n",
    "                print(pe)\n",
    "            df_error = pd.DataFrame([[file_name_item.file_name]],columns=['file_name'])\n",
    "            df_error.to_csv('temp_error.csv',mode='a',index=False,header=False)\n",
    "    time.sleep(120)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
